{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Predictions w/ Trained models and packaging in CSVs for Testing.\n",
    "\n",
    "\n",
    "`-- Leo Lonzarich, 28 May 2023 // [Revised 2-6-23] --`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from pandas_datareader import data as pdr\n",
    "import yfinance as yfin\n",
    "import datetime as dt\n",
    "import pickle\n",
    "import tqdm\n",
    "import os\n",
    "\n",
    "# Yahoo API may have broken previous versions of pd_datareader,\n",
    "# so this is a workaround.\n",
    "yfin.pdr_override()\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "2777 unique points loaded with attributes: \n",
      " Index(['Open', 'High', 'Low', 'Close', 'AdjClose', 'Volume'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Yahoo Finance stock scraping.\n",
    "# **Careful with how many times you run this to avoid IP ban**\n",
    "\n",
    "STOCKS = ['SPY', 'AAPL', 'PG', 'PFE', 'LMT', 'XOM'] # Stocks to predict.\n",
    "START = dt.datetime(2012, 1, 1) # First day of model's training window\n",
    "END = dt.datetime(2023, 1, 15) # Last day of training window\n",
    "NUMDAYS = 40 # The number of days we want our model to predict.\n",
    "\n",
    "TICKER = STOCKS[1] # We run one stock at a time.\n",
    "\n",
    "stock = pdr.get_data_yahoo(TICKER, START, END).rename(columns= {'Adj Close': 'AdjClose'})\n",
    "print(stock.shape[0], \"unique points loaded with attributes: \\n\", stock.\n",
    "      keys())\n",
    "stock = stock.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_to_supervised(data, n_in=5, n_out=1, dropnan=True):\n",
    "    \"\"\"\n",
    "    Frame a time series as a supervised learning dataset.\n",
    "    Arguments:\n",
    "    data: Sequence of observations as a list or NumPy array.\n",
    "    n_in: Number of lag observations as input (X).\n",
    "    n_out: Number of observations as output (y).\n",
    "    dropnan: Boolean whether or not to drop rows with NaN values.\n",
    "    Returns:\n",
    "    Pandas DataFrame of series framed for supervised learning.\n",
    "    \"\"\"\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "    if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "    else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    \n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "        \n",
    "    return agg\n",
    "\n",
    "\n",
    "def data_split(data, lag=60, days=1, train_ratio=0.70,\n",
    "               validation=False, backtest=False):\n",
    "    \"\"\"\n",
    "    Prepping stock data for neural net; scaling down \n",
    "    values and making train-test split.\n",
    "    data: DataFrame, all stock data.\n",
    "    lag: int, number of days used for prediction.\n",
    "    days: int, number of days to predict.\n",
    "    train_ratio: float, percentage of data for training.\n",
    "    validation: bool, split data into train/valid/test when True.\n",
    "    backtest: bool, only performs x-y split when True.\n",
    "    Returns\n",
    "        X_train: array, independent training features.\n",
    "        y_train: array, objective training feature.\n",
    "        X_test: array, independent test features.\n",
    "        y_test: array, objective test feature.\n",
    "        X_valid: array, independent validation features.\n",
    "        y_valid: array, objective validation feature.\n",
    "        X: array, independent features.\n",
    "        y: array, target feature.\n",
    "    \"\"\"\n",
    "    # Selecting 'AdjClose' prices as input and target feature for time series.\n",
    "    data_adj = data.filter(['AdjClose']).values\n",
    "\n",
    "    # Scaling data. Ensures quicker convergence to solution.\n",
    "    scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    scaled_data = scaler.fit_transform(data_adj)\n",
    "\n",
    "    # Splitting input features and target object, X and y.\n",
    "    supervised_data = series_to_supervised(scaled_data, n_in=lag, n_out=days)\n",
    "    X = supervised_data.loc[:, supervised_data.columns != 'var1(t)'] \n",
    "    y = supervised_data['var1(t)'] # Isolating target object.\n",
    "\n",
    "    # Selecting converted data for train-test split.\n",
    "    len_training = int(np.ceil(len(scaled_data) * train_ratio))\n",
    "\n",
    "    X_train = X.iloc[0:len_training].to_numpy()\n",
    "    y_train = y.iloc[0:len_training].to_numpy()\n",
    "    # X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "\n",
    "    # Making validation/test split.\n",
    "    if validation:\n",
    "        len_valid = int((len(X) - len_training)/2)\n",
    "        len_valid += len_training-60\n",
    "\n",
    "        # We subtract lag days since they are needed to actually  \n",
    "        X_valid = X.iloc[len_training-60:len_valid].to_numpy()\n",
    "        y_valid = data_adj[len_training:len_valid]\n",
    "\n",
    "        X_test = X.iloc[len_valid-60:].to_numpy()\n",
    "        y_test = data_adj[len_valid:]\n",
    "\n",
    "    else:\n",
    "        X_test = X.iloc[len_training-60:].to_numpy()\n",
    "        y_test = data_adj[len_training:]\n",
    "\n",
    "    # Reshaping to obtain 3D reps (currently 2d) to pass into LSTM.\n",
    "    # LSTM expects d1 # of samples, d2 # of timesteps, and d3 # of features.\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0],\n",
    "                                   X_train.shape[1], 1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0],\n",
    "                                 X_test.shape[1], 1))\n",
    "\n",
    "    if len(X_test) != len(y_test):\n",
    "        raise Warning('X, y length mismatch.')\n",
    "    \n",
    "    if validation:\n",
    "        X_valid = np.reshape(X_valid, (X_valid.shape[0],\n",
    "                                       X_valid.shape[1], 1))\n",
    "        return X_train, y_train, X_valid, y_valid, X_test, y_test, scaler\n",
    "    \n",
    "    elif backtest:\n",
    "        return X, y, scaler\n",
    "\n",
    "    elif not backtest and not validation:\n",
    "        return X_train, y_train, X_test, y_test, scaler\n",
    "    \n",
    "    else:\n",
    "        ValueError(\n",
    "            \"Cannot simultaneously perform 'backtest' and 'validation'.\"\n",
    "            )\n",
    "        exit()\n",
    "\n",
    "\n",
    "def create_model(trial, in_shape):\n",
    "    '''\n",
    "    A modification on a vanilla model function, where in\n",
    "    this case we pass a trial object that Optuna uses both in\n",
    "    its optimization routine, and for passing values for the\n",
    "    hyperparameters in the case of model fitting.\n",
    "    in_shape: int, gives the col shape (# of features) that the\n",
    "        first LSTM node should expect to receive.\n",
    "    '''\n",
    "    units = trial.suggest_int('units', 64, 150, step=2)\n",
    "    dropout = trial.suggest_float('dropout', 0, 1)\n",
    "    classes = trial.suggest_int('classes', 13, 50, step=1)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-1, log=True)\n",
    "    # activation = trial.suggest_categorical('dense_activation', [None, 'tanh', 'sigmoid'])\n",
    "    # recurrent_dropout = trial.suggest_float('recurrent_droupout', 0, 1)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(\n",
    "        LSTM(\n",
    "            units=units,\n",
    "            activation='tanh',\n",
    "            recurrent_activation='sigmoid',\n",
    "            unroll=False,\n",
    "            use_bias=True,\n",
    "            dropout=dropout,\n",
    "            # recurrent_dropout=recurrent_dropout,\n",
    "            return_sequences=True,\n",
    "            input_shape=(in_shape, 1)\n",
    "        )\n",
    "    )\n",
    "    model.add(\n",
    "        LSTM(\n",
    "            units=int(units/2),\n",
    "            activation='tanh',\n",
    "            recurrent_activation='sigmoid',\n",
    "            unroll=False,\n",
    "            use_bias=True,\n",
    "            dropout=dropout,\n",
    "            # recurrent_dropout=recurrent_dropout,\n",
    "            return_sequences=False,\n",
    "        )\n",
    "    )\n",
    "    model.add(\n",
    "        Dense(\n",
    "            classes,\n",
    "            activation=None,\n",
    "            use_bias=True\n",
    "        )\n",
    "    )\n",
    "    model.add(\n",
    "        Dense(\n",
    "            1,\n",
    "            activation= None, # activation,\n",
    "            use_bias=True\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Only use 'accuracy' metric for classification.\n",
    "    model.compile(\n",
    "        loss='mean_squared_error',\n",
    "        optimizer=Adam(\n",
    "            learning_rate=learning_rate,\n",
    "            beta_1=0.9,\n",
    "            beta_2=0.999,\n",
    "            epsilon=1e-07\n",
    "        ),\n",
    "        metrics=['mean_squared_error'] # ['mean_absolute_percentage_error']\n",
    "    )\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAG = 60 # Number of days to use for predicting the following day(s).\n",
    "DAYS = 1 # Number of days to predict with each lag period.\n",
    "\n",
    "UNITS = 150\n",
    "CLASSES = 50\n",
    "BATCHSIZE= 128\n",
    "EPOCHS = 10\n",
    "\n",
    "split = data_split(stock, lag=LAG, days=DAYS, train_ratio=1)\n",
    "\n",
    "X_train, y_train = split[0], split[1]\n",
    "scaler = split[4]\n",
    "\n",
    "if len(X_train) != len(stock) - LAG:\n",
    "    raise ValueError('X_train is incorrectly formatted. Check input Stock Data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of finished trials: 40\n",
      "Best trial:\n",
      "    RMSE Value: 3.818e+00\n",
      "    Params: \n",
      "    batchsize: 2.500e+01\n",
      "    epochs: 8.000e+00\n",
      "    units: 1.340e+02\n",
      "    dropout: 3.478e-03\n",
      "    classes: 3.000e+01\n",
      "    learning_rate: 2.374e-03\n"
     ]
    }
   ],
   "source": [
    "PATH = 'OptStudy_' + TICKER\n",
    "PATH = 'mape6'\n",
    "filepath = '/Users/leoglonz/Desktop/stock_analysis/opt_cache/' + PATH + '.pickle'\n",
    "\n",
    "\n",
    "study = pickle.load(open(filepath, 'rb'))\n",
    "print(\"Number of finished trials: %i\" %len(study.trials))\n",
    "print(\"Best trial:\")\n",
    "\n",
    "best_trial = study.best_trial\n",
    "\n",
    "print(\"    RMSE Value: %.3e\" %best_trial.value)\n",
    "print(\"    Params: \")\n",
    "for key, value in best_trial.params.items():\n",
    "    if value == str(value) or value == None:\n",
    "        print(\"    %s: %s\" %(key, value))\n",
    "    else:\n",
    "        print(\"    %s: %.3e\" %(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-02 11:36:44.224444: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-06-02 11:36:44.517497: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-06-02 11:36:44.761922: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-06-02 11:36:45.071061: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-06-02 11:36:45.487256: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109/109 [==============================] - 7s 48ms/step - loss: 0.0049 - mean_squared_error: 0.0049\n",
      "Epoch 2/8\n",
      "109/109 [==============================] - 4s 40ms/step - loss: 3.7878e-04 - mean_squared_error: 3.7878e-04\n",
      "Epoch 3/8\n",
      "109/109 [==============================] - 4s 40ms/step - loss: 3.3323e-04 - mean_squared_error: 3.3323e-04\n",
      "Epoch 4/8\n",
      "109/109 [==============================] - 4s 41ms/step - loss: 2.8963e-04 - mean_squared_error: 2.8963e-04\n",
      "Epoch 5/8\n",
      "109/109 [==============================] - 5s 41ms/step - loss: 2.9365e-04 - mean_squared_error: 2.9365e-04\n",
      "Epoch 6/8\n",
      "109/109 [==============================] - 4s 41ms/step - loss: 3.4109e-04 - mean_squared_error: 3.4109e-04\n",
      "Epoch 7/8\n",
      "109/109 [==============================] - 4s 40ms/step - loss: 3.2288e-04 - mean_squared_error: 3.2288e-04\n",
      "Epoch 8/8\n",
      "109/109 [==============================] - 4s 40ms/step - loss: 2.6290e-04 - mean_squared_error: 2.6290e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x30c888910>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_shape = X_train.shape[1] # Number of input features.\n",
    "model = create_model(best_trial, in_shape)\n",
    "\n",
    "best_trial.params\n",
    "model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        shuffle=True,\n",
    "        batch_size=best_trial.params['batchsize'],\n",
    "        epochs=best_trial.params['epochs'],\n",
    "        verbose=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_4 (LSTM)               (None, 60, 134)           72896     \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 67)                54136     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 30)                2040      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 31        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 129,103\n",
      "Trainable params: 129,103\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "61 unique points loaded with attributes: \n",
      " Index(['Date', 'Open', 'High', 'Low', 'Close', 'AdjClose', 'Volume'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Fetching available test period data. What data does not exist at time\n",
    "# of writing (i.e., after 31 May) will be predicted manually.\n",
    "\n",
    "START = dt.datetime(2023, 3, 3)\n",
    "END = dt.datetime(2023, 5, 31) #dt.datetime(2023, 8, 1)\n",
    "\n",
    "\n",
    "test_stock = pdr.get_data_yahoo(TICKER, START, END).rename(columns= {'Adj Close': 'AdjClose'})\n",
    "print(test_stock.shape[0], \"unique points loaded with attributes: \\n\", stock.\n",
    "      keys())\n",
    "test_stock = test_stock.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]2023-06-02 11:48:13.790545: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-06-02 11:48:13.889369: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-06-02 11:48:14.031006: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "100%|██████████| 40/40 [00:02<00:00, 18.06it/s]\n"
     ]
    }
   ],
   "source": [
    "# Making predictions on Test Period data. \n",
    "\n",
    "y_preds = np.zeros([NUMDAYS])\n",
    "\n",
    "for i in tqdm.tqdm(range(NUMDAYS)):\n",
    "    if i == 0: \n",
    "        # Generating first instance of test data.\n",
    "        split = data_split(test_stock, lag=LAG, days=DAYS, train_ratio=1)\n",
    "        X_test = split[0]\n",
    "\n",
    "        if len(X_test) != len(test_stock) - LAG:\n",
    "            raise ValueError('X_test is incorrectly formatted. Check input Stock Data.')\n",
    "        \n",
    "        # print(\"Starting with day %i prediction.\" %len(X_test))\n",
    "\n",
    "    else:\n",
    "        # Update test data; drop 1st value, append latest prediction to end.\n",
    "        X_test = np.append(X_test[0][1:], pred, axis=0).reshape((1,60,1))\n",
    "    \n",
    "    pred = model.predict(X_test, verbose=0)\n",
    "    y_preds[i] = pred\n",
    "\n",
    "y_preds = scaler.inverse_transform(y_preds.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepping and offloading to CSV.\n",
    "\n",
    "dates = pd.bdate_range(start='5/31/2023', end='7/25/2023')\n",
    "\n",
    "predictions = pd.DataFrame({\n",
    "    'Date': np.array(dates).T,\n",
    "    'Predictions': y_preds.reshape(len(y_preds))}\n",
    "    )\n",
    "\n",
    "\n",
    "outname = TICKER + '_1mo_0623_preds.csv'\n",
    "outdir = './Lit_Predictions'\n",
    "\n",
    "if not os.path.exists(outdir):\n",
    "    os.mkdir(outdir)\n",
    "\n",
    "fullname = os.path.join(outdir, outname)   \n",
    "\n",
    "predictions.to_csv(fullname, sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>AdjClose</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-03-03</td>\n",
       "      <td>148.039993</td>\n",
       "      <td>151.110001</td>\n",
       "      <td>147.330002</td>\n",
       "      <td>151.029999</td>\n",
       "      <td>150.821381</td>\n",
       "      <td>70732300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-03-06</td>\n",
       "      <td>153.789993</td>\n",
       "      <td>156.300003</td>\n",
       "      <td>153.460007</td>\n",
       "      <td>153.830002</td>\n",
       "      <td>153.617523</td>\n",
       "      <td>87558000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-03-07</td>\n",
       "      <td>153.699997</td>\n",
       "      <td>154.029999</td>\n",
       "      <td>151.130005</td>\n",
       "      <td>151.600006</td>\n",
       "      <td>151.390610</td>\n",
       "      <td>56182000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-03-08</td>\n",
       "      <td>152.809998</td>\n",
       "      <td>153.470001</td>\n",
       "      <td>151.830002</td>\n",
       "      <td>152.869995</td>\n",
       "      <td>152.658844</td>\n",
       "      <td>47204800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-03-09</td>\n",
       "      <td>153.559998</td>\n",
       "      <td>154.539993</td>\n",
       "      <td>150.229996</td>\n",
       "      <td>150.589996</td>\n",
       "      <td>150.381989</td>\n",
       "      <td>53833600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>2023-05-23</td>\n",
       "      <td>173.130005</td>\n",
       "      <td>173.380005</td>\n",
       "      <td>171.279999</td>\n",
       "      <td>171.559998</td>\n",
       "      <td>171.559998</td>\n",
       "      <td>50747300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>2023-05-24</td>\n",
       "      <td>171.089996</td>\n",
       "      <td>172.419998</td>\n",
       "      <td>170.520004</td>\n",
       "      <td>171.839996</td>\n",
       "      <td>171.839996</td>\n",
       "      <td>45143500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>2023-05-25</td>\n",
       "      <td>172.410004</td>\n",
       "      <td>173.899994</td>\n",
       "      <td>171.690002</td>\n",
       "      <td>172.990005</td>\n",
       "      <td>172.990005</td>\n",
       "      <td>56058300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>2023-05-26</td>\n",
       "      <td>173.320007</td>\n",
       "      <td>175.770004</td>\n",
       "      <td>173.110001</td>\n",
       "      <td>175.429993</td>\n",
       "      <td>175.429993</td>\n",
       "      <td>54835000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>2023-05-30</td>\n",
       "      <td>176.960007</td>\n",
       "      <td>178.990005</td>\n",
       "      <td>176.570007</td>\n",
       "      <td>177.300003</td>\n",
       "      <td>177.300003</td>\n",
       "      <td>55964400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date        Open        High         Low       Close    AdjClose  \\\n",
       "0  2023-03-03  148.039993  151.110001  147.330002  151.029999  150.821381   \n",
       "1  2023-03-06  153.789993  156.300003  153.460007  153.830002  153.617523   \n",
       "2  2023-03-07  153.699997  154.029999  151.130005  151.600006  151.390610   \n",
       "3  2023-03-08  152.809998  153.470001  151.830002  152.869995  152.658844   \n",
       "4  2023-03-09  153.559998  154.539993  150.229996  150.589996  150.381989   \n",
       "..        ...         ...         ...         ...         ...         ...   \n",
       "56 2023-05-23  173.130005  173.380005  171.279999  171.559998  171.559998   \n",
       "57 2023-05-24  171.089996  172.419998  170.520004  171.839996  171.839996   \n",
       "58 2023-05-25  172.410004  173.899994  171.690002  172.990005  172.990005   \n",
       "59 2023-05-26  173.320007  175.770004  173.110001  175.429993  175.429993   \n",
       "60 2023-05-30  176.960007  178.990005  176.570007  177.300003  177.300003   \n",
       "\n",
       "      Volume  \n",
       "0   70732300  \n",
       "1   87558000  \n",
       "2   56182000  \n",
       "3   47204800  \n",
       "4   53833600  \n",
       "..       ...  \n",
       "56  50747300  \n",
       "57  45143500  \n",
       "58  56058300  \n",
       "59  54835000  \n",
       "60  55964400  \n",
       "\n",
       "[61 rows x 7 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-05-31</td>\n",
       "      <td>151.449783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-06-01</td>\n",
       "      <td>151.198529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-06-02</td>\n",
       "      <td>150.237218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-06-05</td>\n",
       "      <td>148.899704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-06-06</td>\n",
       "      <td>147.379644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2023-06-07</td>\n",
       "      <td>145.785003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2023-06-08</td>\n",
       "      <td>144.173530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2023-06-09</td>\n",
       "      <td>142.574855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2023-06-12</td>\n",
       "      <td>141.003301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2023-06-13</td>\n",
       "      <td>139.465133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2023-06-14</td>\n",
       "      <td>137.962449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2023-06-15</td>\n",
       "      <td>136.495267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2023-06-16</td>\n",
       "      <td>135.062465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2023-06-19</td>\n",
       "      <td>133.662486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2023-06-20</td>\n",
       "      <td>132.293494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2023-06-21</td>\n",
       "      <td>130.953631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2023-06-22</td>\n",
       "      <td>129.640980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2023-06-23</td>\n",
       "      <td>128.353704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2023-06-26</td>\n",
       "      <td>127.090067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2023-06-27</td>\n",
       "      <td>125.848443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2023-06-28</td>\n",
       "      <td>124.627346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2023-06-29</td>\n",
       "      <td>123.425361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2023-06-30</td>\n",
       "      <td>122.241293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2023-07-03</td>\n",
       "      <td>121.074029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2023-07-04</td>\n",
       "      <td>119.922563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2023-07-05</td>\n",
       "      <td>118.786034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2023-07-06</td>\n",
       "      <td>117.663618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2023-07-07</td>\n",
       "      <td>116.554642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2023-07-10</td>\n",
       "      <td>115.458454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2023-07-11</td>\n",
       "      <td>114.374422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2023-07-12</td>\n",
       "      <td>113.302094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2023-07-13</td>\n",
       "      <td>112.241018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2023-07-14</td>\n",
       "      <td>111.190723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2023-07-17</td>\n",
       "      <td>110.150847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2023-07-18</td>\n",
       "      <td>109.120988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2023-07-19</td>\n",
       "      <td>108.100856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2023-07-20</td>\n",
       "      <td>107.090109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2023-07-21</td>\n",
       "      <td>106.088456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2023-07-24</td>\n",
       "      <td>105.095617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2023-07-25</td>\n",
       "      <td>104.111279</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  Predictions\n",
       "0  2023-05-31   151.449783\n",
       "1  2023-06-01   151.198529\n",
       "2  2023-06-02   150.237218\n",
       "3  2023-06-05   148.899704\n",
       "4  2023-06-06   147.379644\n",
       "5  2023-06-07   145.785003\n",
       "6  2023-06-08   144.173530\n",
       "7  2023-06-09   142.574855\n",
       "8  2023-06-12   141.003301\n",
       "9  2023-06-13   139.465133\n",
       "10 2023-06-14   137.962449\n",
       "11 2023-06-15   136.495267\n",
       "12 2023-06-16   135.062465\n",
       "13 2023-06-19   133.662486\n",
       "14 2023-06-20   132.293494\n",
       "15 2023-06-21   130.953631\n",
       "16 2023-06-22   129.640980\n",
       "17 2023-06-23   128.353704\n",
       "18 2023-06-26   127.090067\n",
       "19 2023-06-27   125.848443\n",
       "20 2023-06-28   124.627346\n",
       "21 2023-06-29   123.425361\n",
       "22 2023-06-30   122.241293\n",
       "23 2023-07-03   121.074029\n",
       "24 2023-07-04   119.922563\n",
       "25 2023-07-05   118.786034\n",
       "26 2023-07-06   117.663618\n",
       "27 2023-07-07   116.554642\n",
       "28 2023-07-10   115.458454\n",
       "29 2023-07-11   114.374422\n",
       "30 2023-07-12   113.302094\n",
       "31 2023-07-13   112.241018\n",
       "32 2023-07-14   111.190723\n",
       "33 2023-07-17   110.150847\n",
       "34 2023-07-18   109.120988\n",
       "35 2023-07-19   108.100856\n",
       "36 2023-07-20   107.090109\n",
       "37 2023-07-21   106.088456\n",
       "38 2023-07-24   105.095617\n",
       "39 2023-07-25   104.111279"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
